{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ee371612-ace1-4dee-b383-2ff875581537",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (3.8.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.10/site-packages (from nltk) (2023.10.3)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from nltk) (8.1.3)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from nltk) (4.64.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c7914bf4-7a93-4693-915b-9f48bded66f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "import nltk\n",
    "import shutil\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup as soup\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1be695f8-0d04-4149-867f-5e2ef3e178de",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('~/work/Input.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3d1524b4-718d-4da3-8e6f-9c36cd9fd804",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL_ID</th>\n",
       "      <th>URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>123.0</td>\n",
       "      <td>https://insights.blackcoffer.com/rise-of-telem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>321.0</td>\n",
       "      <td>https://insights.blackcoffer.com/rise-of-e-hea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2345.0</td>\n",
       "      <td>https://insights.blackcoffer.com/rise-of-e-hea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4321.0</td>\n",
       "      <td>https://insights.blackcoffer.com/rise-of-telem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>432.0</td>\n",
       "      <td>https://insights.blackcoffer.com/rise-of-telem...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   URL_ID                                                URL\n",
       "0   123.0  https://insights.blackcoffer.com/rise-of-telem...\n",
       "1   321.0  https://insights.blackcoffer.com/rise-of-e-hea...\n",
       "2  2345.0  https://insights.blackcoffer.com/rise-of-e-hea...\n",
       "3  4321.0  https://insights.blackcoffer.com/rise-of-telem...\n",
       "4   432.0  https://insights.blackcoffer.com/rise-of-telem..."
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "58ff590a-aca3-4f60-b097-772242175197",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_ids = df.URL_ID.values\n",
    "urls = df.URL.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0a4e23de-81c8-4d1e-8199-c74d1877a196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://insights.blackcoffer.com/rise-of-telemedicine-and-its-impact-on-livelihood-by-2040-3-2/\n"
     ]
    }
   ],
   "source": [
    "print(urls[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "be5689ed-1d71-4c60-8199-0b269b6d92e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(id_, url):\n",
    "    page = urlopen(url)\n",
    "    page_soup = soup(page, \"html.parser\")\n",
    "    title = page_soup.title.string\n",
    "    texts = ''\n",
    "    for txt in page_soup.article.find_all('p', text=True):\n",
    "        texts += (' ' + txt.text)\n",
    "    texts = texts.strip()\n",
    "    return title, texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f3b0e297-611b-458b-845b-68283e9d28c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 25/114 [01:15<04:10,  2.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11668.0 https://insights.blackcoffer.com/how-neural-networks-can-be-applied-in-various-areas-in-the-future/ HTTP Error 404: Not Found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 38/114 [01:49<02:52,  2.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17671.4 https://insights.blackcoffer.com/covid-19-environmental-impact-for-the-future/ HTTP Error 404: Not Found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 114/114 [04:57<00:00,  2.61s/it]\n"
     ]
    }
   ],
   "source": [
    "for id_, url in tqdm(zip(url_ids, urls), total=urls.shape[0]):\n",
    "    try:\n",
    "        title, texts = extract(id_, url)\n",
    "        if os.path.exists('./output_txt/' + str(id_)+'.txt'):\n",
    "            continue\n",
    "        with open('./output_txt/' + str(id_)+'.txt', 'w') as f:\n",
    "            f.write(title)\n",
    "            f.write('\\n')\n",
    "            f.write(texts)\n",
    "    except Exception as e:\n",
    "        print(id_, url, e)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "063769d5-b81b-4cb2-935b-f4dfa678aff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_files = glob.glob(\"./output_txt/*.txt\")\n",
    "stop_files = glob.glob(\"./stopwords/*.txt\")\n",
    "dict_files = glob.glob(\"./master_dict/*.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2ec95bad-47b8-4d29-8a8b-f116873d7ef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12765\n"
     ]
    }
   ],
   "source": [
    "stopword_list = []\n",
    "for file in stop_files:\n",
    "    df = pd.read_csv(file, sep='|', encoding='latin-1', header=None)\n",
    "    stopword_list += list(df[0].values)\n",
    "stopword_list = set([str(x).lower() for x in stopword_list])\n",
    "print(len(stopword_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f8b9d0a6-122e-45f9-b107-17b9381ede0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2006\n",
      "4783\n"
     ]
    }
   ],
   "source": [
    "positive_list = []\n",
    "negative_list = []\n",
    "for file in dict_files:\n",
    "    df = pd.read_csv(file, encoding='latin-1', header=None)\n",
    "    if 'positive' in file:\n",
    "        positive_list += list(df[0].values)\n",
    "    else:\n",
    "        negative_list += list(df[0].values)\n",
    "\n",
    "positive_list = set([str(x).lower() for x in positive_list]) \n",
    "negative_list = set([str(x).lower() for x in negative_list])\n",
    "print(len(positive_list))\n",
    "print(len(negative_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "451bfecf-bfeb-45b5-8032-2ffc8c24fd51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_texts(text, stopword_list):\n",
    "    tokens = word_tokenize(text)\n",
    "    words = [tok for tok in tokens if tok.lower() not in stopword_list]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "add253d3-8627-4f28-88b4-080f4e44c16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculate_score(words, positive_list, negative_list):\n",
    "    positive_score = 0\n",
    "    negative_score = 0\n",
    "    for i in words:\n",
    "        if i in  positive_list:\n",
    "            positive_score +=1\n",
    "        if i in  negative_list:\n",
    "            negative_score +=1\n",
    "    \n",
    "    polarity_score = (positive_score - negative_score)/(positive_score + negative_score + 1e-5)\n",
    "    subjectivity_score = (positive_score + negative_score)/(len(words) + 1e-5)\n",
    "    return positive_score,negative_score,polarity_score,subjectivity_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4ee55d4d-f257-4b85-a4e3-8538b66ff581",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readability_analysis(texts):\n",
    "    sent = texts.split('.')\n",
    "    words = texts.split(' ')\n",
    "    avg_sent_len = len(words) / len(sent)\n",
    "    comp_w = 0\n",
    "    sum_comp = 0\n",
    "    for word in words:\n",
    "        count = count_syllables(word)\n",
    "        if count > 2:\n",
    "            comp_w += 1\n",
    "        sum_comp += count\n",
    "    \n",
    "    syll_per_word = sum_comp / len(words)\n",
    "    perc_comp_word = comp_w / len(words)\n",
    "    fog = 0.4 * (avg_sent_len + perc_comp_word)\n",
    "    return syll_per_word, avg_sent_len, comp_w, fog, perc_comp_word\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e20c8fdb-2354-487d-a65e-43f16b2b8db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_syllables(word):\n",
    "    count = 0\n",
    "    word = re.sub(r'[^\\w\\s]','',word)\n",
    "    word = re.sub(r'_','',word)\n",
    "    word = word.strip()\n",
    "    for c in word:\n",
    "        if c.lower() in ['a', 'e', 'i', 'o', 'u']:\n",
    "            count += 1\n",
    "    if word[-2:].lower() in ['es', 'ed']:\n",
    "        count -= 1\n",
    "    return count\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "64a4c3c2-c77e-4d54-9a07-7d1bb0379a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_pronoun(texts):\n",
    "    pronounRegex = re.compile(r'\\b(I|we|my|ours|(?-i:us))\\b',re.I)\n",
    "    pronouns = pronounRegex.findall(texts)\n",
    "    return len(pronouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "183034e8-e29f-4f46-b758-aaba4c2b1937",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_avg_word_len(words):\n",
    "    char_cnt = 0\n",
    "    for w in words:\n",
    "        char_cnt += len(w)\n",
    "    \n",
    "    return char_cnt / len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7cf77cd1-82a6-4030-b18d-71b809b1f8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_count(texts):\n",
    "    from nltk.corpus import stopwords     \n",
    "    nltk_words = list(stopwords.words('english'))\n",
    "    words = clean_texts(texts, nltk_words)\n",
    "    words = [re.sub(r'[^\\w\\s]','', re.sub(r'_','',w)).strip() for w in words]\n",
    "    return len(words), words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "88c45e9d-d586-4bec-bd8b-71a0118e9e58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 112/112 [00:03<00:00, 30.54it/s]\n"
     ]
    }
   ],
   "source": [
    "out = pd.read_csv('~/work/Output data structure.csv')\n",
    "ids = out.URL_ID.astype('str')\n",
    "for file in tqdm(txt_files, total=len(txt_files)):\n",
    "    with open(file) as f:\n",
    "        texts = f.read()\n",
    "    texts = texts.replace('\\n', '. ')\n",
    "    words = clean_texts(texts,stopword_list)\n",
    "    positive_score,negative_score,polarity_score,subjectivity_score = calculate_score(words, positive_list, negative_list)\n",
    "    syll_per_word, avg_sent_len, comp_w, fog, perc_comp_word = readability_analysis(texts)\n",
    "    num_pronoun = count_pronoun(texts)\n",
    "    count, words = word_count(texts)\n",
    "    avg_word_len = get_avg_word_len(words)\n",
    "    idx = np.where(ids == file.split('/')[-1][:-4])[0]\n",
    "    out.loc[idx, 'POSITIVE SCORE'] = positive_score\n",
    "    out.loc[idx, 'NEGATIVE SCORE'] = negative_score\n",
    "    out.loc[idx, 'POLARITY SCORE'] = polarity_score\n",
    "    out.loc[idx, 'SUBJECTIVITY SCORE'] = subjectivity_score\n",
    "    out.loc[idx, 'FOG INDEX'] = fog\n",
    "    out.loc[idx, 'AVG NUMBER OF WORDS PER SENTENCE'] = avg_sent_len\n",
    "    out.loc[idx, 'PERCENTAGE OF COMPLEX WORDS'] = perc_comp_word\n",
    "    out.loc[idx, 'AVG SENTENCE LENGTH'] = avg_sent_len\n",
    "    out.loc[idx, 'SYLLABLE PER WORD'] = syll_per_word\n",
    "    out.loc[idx, 'COMPLEX WORD COUNT'] = comp_w\n",
    "    out.loc[idx, 'PERSONAL PRONOUNS'] = num_pronoun\n",
    "    out.loc[idx, 'AVG WORD LENGTH'] = avg_word_len\n",
    "    out.loc[idx, 'WORD COUNT'] = count\n",
    "\n",
    "out.to_csv('./final_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22dd1ca-ebdc-4416-9fac-fb2ea4b281b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0512e7be-0b67-4ae5-939d-43e380d3234c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
